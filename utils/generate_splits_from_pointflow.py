import argparse
import json
import os
from pathlib import Path
from typing import Set, Tuple


def load_paths(split_path: str) -> Set[Tuple[str, str]]:
    with open(split_path) as f:
        split_data = json.load(f)

    output = []
    for sample in split_data:  # type: str
        shape_class, shape_object = sample.split("/")[1:]
        output.append((shape_class, shape_object))
    return set(output)


def generate_splits(
    data_folder: str,
    original_train_split: str,
    original_valid_split: str,
    original_test_split: str,
):
    train_set = load_paths(original_train_split)
    valid_set = load_paths(original_valid_split)
    test_set = load_paths(original_test_split)

    output_train_set = []
    output_valid_set = []
    output_test_set = []

    def check_object_exists(object_cls: str, inst_obj: str) -> bool:
        data_tuple = object_cls, inst_obj
        return (
            data_tuple in train_set
            or data_tuple in valid_set
            or data_tuple in test_set
        )

    not_existing_train_paths = []
    not_existing_valid_paths = []
    not_existing_test_paths = []

    for object_class in os.listdir(data_folder):

        # train
        for instance_object in sorted(
            os.listdir(os.path.join(data_folder, object_class, "train"))
        ):
            instance_object = instance_object.split(".")[0]
            obj_path = f"shape_data/{object_class}/{instance_object}"
            if check_object_exists(object_class, instance_object):
                output_train_set.append(obj_path)
            else:
                not_existing_train_paths.append(obj_path)

        # valid
        for instance_object in sorted(
            os.listdir(os.path.join(data_folder, object_class, "val"))
        ):
            instance_object = instance_object.split(".")[0]
            obj_path = f"shape_data/{object_class}/{instance_object}"
            if check_object_exists(object_class, instance_object):
                output_valid_set.append(obj_path)
            else:
                not_existing_valid_paths.append(obj_path)

        # test
        for instance_object in sorted(
            os.listdir(os.path.join(data_folder, object_class, "test"))
        ):
            instance_object = instance_object.split(".")[0]
            obj_path = f"shape_data/{object_class}/{instance_object}"
            if check_object_exists(object_class, instance_object):
                output_test_set.append(obj_path)
            else:
                not_existing_test_paths.append(obj_path)

    split_folder = Path(original_train_split).parent

    with open(split_folder / "pointflow_train_file_list.json", "w") as f:
        json.dump(output_train_set, f)

    with open(split_folder / "pointflow_val_file_list.json", "w") as f:
        json.dump(output_valid_set, f)

    with open(split_folder / "pointflow_test_file_list.json", "w") as f:
        json.dump(output_test_set, f)

    print(
        f"Lacking {len(not_existing_train_paths)} train from the PointNet "
        f"dataset"
    )
    print(
        f"Lacking {len(not_existing_valid_paths)} valid from the PointNet "
        f"dataset"
    )
    print(
        f"Lacking {len(not_existing_test_paths)} test from the PointNet "
        f"dataset"
    )

    print("New splits are located in {}".format(split_folder.as_posix()))
    print(
        "CAUTION: Lack of paths is caused by the fact that PointFlow "
        "consists of more data samples than the PointNet was trained on"
    )


def main():
    parser = argparse.ArgumentParser()

    parser.add_argument(
        "--data_folder",
        help=(
            "Path to the dataset folder generated by the original PointFlow "
            "dataset"
        ),
        type=str,
        required=True,
    )

    parser.add_argument(
        "--original_train_split",
        type=str,
        required=True,
        help="Path to the original file containing split of the training data",
    )
    parser.add_argument(
        "--original_valid_split",
        type=str,
        required=True,
        help="Path to the original file containing split of the valid data",
    )
    parser.add_argument(
        "--original_test_split",
        type=str,
        required=True,
        help="Path to the original file containing split of the test data",
    )

    args = parser.parse_args()
    generate_splits(**vars(args))


if __name__ == "__main__":
    main()
